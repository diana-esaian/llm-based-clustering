{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Optimize the following prompt to make it more understandable for an LLM. Improve clarity, specificity, and alignment with best practices to ensure high-quality responses. The prompt should be concise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_edit_1 = \"\"\"You are an expert in extracting the topics of concern from the customer messages. \n",
    "I will give you a number of messages, you need to extract the topics and list the topic tags for each message.\n",
    "Your output should be of the following format:\n",
    "{'1': ['topic1', 'topic2', ...],'2': ['topic1', 'topic2', ...], ...}\n",
    "Do not include any other words or symbols in the output.\n",
    "\"\"\"\n",
    "\n",
    "edited_prompt_1 = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt_to_edit_1,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction = system_prompt\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(edited_prompt_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_edit_2 = \"\"\"You are a clustering expert. You are asked to cluster the following subtopics into more general clusters. You need to return a python dictionary of the following format:\n",
    "{\"Cluster1\":[\"subtopic1\", \"subtopic2\", ...], \"Cluster2\":[\"subtopic1\", \"subtopic2\", ...], ...}\n",
    "Make sure that each subtopic should be included in only one cluster.\n",
    "Make sure that all the subtopics are included in the clusters.\n",
    "\"\"\"\n",
    "\n",
    "edited_prompt_2 = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt_to_edit_2,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction = system_prompt\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(edited_prompt_2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_edit_3 = \"\"\"You are provided with a list of clusters. If you find that there are clusters that can be merged into one, return them. \n",
    "Your response should be of the following format:\n",
    "{\"NewClusterName\": [cluster1_to_merge, cluster2_to_merge, ...], NewClusterName2\": [cluster4_to_merge, cluster6_to_merge, ...]}\"\"\"\n",
    "\n",
    "edited_prompt_3 = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt_to_edit_3,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction = system_prompt\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(edited_prompt_3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_edit_4 = \"\"\"You will be given a cluster with the list of question topics included in it. \n",
    "You need to write a one sentence long description that clearly explanation of what kinds of questions belong to this cluster.\n",
    "If needed, you can rewrite the name of the cluster to make it more representative. Note it should be short.\n",
    "Your response should be in the following format:\n",
    "{\"name\": \"<cluster_name>\", \"description\": \"<description>\"}\"\"\"\n",
    "\n",
    "edited_prompt_4 = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt_to_edit_4,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction = system_prompt\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(edited_prompt_4.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
